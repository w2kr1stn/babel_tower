services:
  stt:
    image: ghcr.io/speaches-ai/speaches:latest-cuda
    container_name: babel-tower-stt
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "29000:8000"
    volumes:
      - stt-models:/home/ubuntu/.cache/huggingface/hub
    environment:
      - WHISPER__MODEL=${BABEL_STT_MODEL:-Systran/faster-whisper-large-v3}
      - WHISPER__COMPUTE_TYPE=int8
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  daemon:
    build:
      context: ..
      dockerfile: docker/Dockerfile.app
    container_name: babel-tower-daemon
    command: ["python", "-m", "babel_tower.cli", "daemon"]
    environment:
      - PULSE_SERVER=unix:/tmp/pulse.socket
      - BABEL_STT_URL=http://stt:8000
      - BABEL_LLM_URL=http://ai-station:4000
      - BABEL_DEFAULT_MODE=strukturieren
      - WAYLAND_DISPLAY=${WAYLAND_DISPLAY:-wayland-0}
    volumes:
      - ${XDG_RUNTIME_DIR:-/run/user/1000}/pulse/native:/tmp/pulse.socket
      - ${XDG_RUNTIME_DIR:-/run/user/1000}/${WAYLAND_DISPLAY:-wayland-0}:/tmp/${WAYLAND_DISPLAY:-wayland-0}
    user: "${UID:-1000}:${GID:-1000}"
    depends_on:
      stt:
        condition: service_healthy
    restart: unless-stopped

volumes:
  stt-models:
