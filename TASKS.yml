feature: "Babel Tower — Voice Input Pipeline for Claude Code"
branch: "feature/babel-tower-voice-pipeline"

waves:
  - name: "Wave 1: Foundation + STT Setup"
    tasks:
      - id: T1
        title: "Fix project scaffolding"
        description: |
          Update pyproject.toml: Python >=3.13, package paths to app/babel_tower/,
          add runtime dependencies (fastmcp, httpx, pydantic-settings, silero-vad,
          sounddevice, soundfile, numpy, typer). Fix hatch build targets, pytest/coverage
          paths. Update .python-version to 3.13. Create app/babel_tower/__init__.py.
          Remove old app/tower/ and main.py placeholder.
        files:
          - "pyproject.toml"
          - ".python-version"
          - "app/babel_tower/__init__.py"
        test: "uv sync && uv run python -c 'import babel_tower'"
        dod: "uv sync succeeds, babel_tower package importable"

      - id: T2
        title: "Configuration module"
        description: |
          Create Pydantic BaseSettings class with BABEL_ env prefix.
          Settings: stt_url, stt_model, stt_language, llm_url, llm_model,
          llm_timeout, audio_sample_rate, audio_channels, vad_threshold,
          silence_duration, default_mode, durchreichen_max_words, prompts_dir.
          Write unit tests for default values and env var overrides.
        files:
          - "app/babel_tower/config.py"
          - "tests/conftest.py"
          - "tests/test_config.py"
        test: "uv run pytest tests/test_config.py"
        dod: "Settings load from env vars, defaults correct, tests pass"

      - id: T3
        title: "STT Docker container (faster-whisper)"
        description: |
          Create Dockerfile.stt based on sterile-cuda-runner pattern
          (nvidia/cuda:12.4.1-base-ubuntu22.04, uv, GPU passthrough).
          Install faster-whisper-server for OpenAI-compatible API.
          Create docker-compose.yml with stt service, GPU reservation,
          model cache volume, port 9000.
        files:
          - "docker/Dockerfile.stt"
          - "docker/docker-compose.yml"
        test: "docker compose -f docker/docker-compose.yml up -d stt && curl -s http://localhost:9000/v1/models"
        dod: "STT container starts with GPU, OpenAI API responds on :9000"

      - id: T4
        title: "STT client module"
        description: |
          Create httpx-based STT client that POSTs WAV bytes to
          /v1/audio/transcriptions (OpenAI-compatible). Accept WAV as
          bytes or BytesIO. Return transcript string. Handle connection
          errors, timeouts, non-200 responses. Write unit tests with
          mocked HTTP responses.
        files:
          - "app/babel_tower/stt.py"
          - "tests/test_stt.py"
        test: "uv run pytest tests/test_stt.py"
        dod: "STT client sends audio, parses response, handles errors, tests pass"

  - name: "Wave 2: Pipeline Core"
    depends_on: ["Wave 1"]
    tasks:
      - id: T6
        title: "Audio capture with VAD"
        description: |
          Record audio from mic via sounddevice (16kHz, mono, int16).
          Use silero-vad (ONNX) for speech detection. Start recording on
          speech onset, stop after configurable silence duration (default 1.5s).
          Return WAV bytes as BytesIO. All parameters from Settings.
        files:
          - "app/babel_tower/audio.py"
          - "tests/test_audio.py"
        test: "uv run pytest tests/test_audio.py"
        dod: "Records audio, detects speech boundaries via VAD, produces WAV bytes"

      - id: T7
        title: "LLM postprocessing module"
        description: |
          Load system prompts from prompts/ directory (strukturieren.md,
          bereinigen.md, durchreichen.md). Auto-select durchreichen for
          transcripts with fewer than durchreichen_max_words. Call M5
          LiteLLM endpoint via httpx POST /v1/chat/completions.
          Create the three system prompt files.
        files:
          - "app/babel_tower/processing.py"
          - "prompts/strukturieren.md"
          - "prompts/bereinigen.md"
          - "prompts/durchreichen.md"
          - "tests/test_processing.py"
        test: "uv run pytest tests/test_processing.py"
        dod: "All 3 modes work, auto-mode selection correct, prompts loaded, tests pass"

      - id: T8
        title: "Output module"
        description: |
          Clipboard output via wl-copy subprocess. Desktop notification
          via notify-send subprocess with text preview (truncated).
          Both operations should be non-blocking and fail gracefully
          if tools are not available.
        files:
          - "app/babel_tower/output.py"
        test: "uv run python -c 'from babel_tower.output import copy_to_clipboard, notify'"
        dod: "Clipboard and notification functions implemented, import works"

      - id: T9
        title: "End-to-end pipeline and CLI"
        description: |
          Create pipeline.py that wires audio → stt → processing → output.
          Pipeline accepts mode override parameter. Create Typer CLI with
          subcommands: listen (one-shot record+process), process (from WAV file).
          Register CLI as babel-tower entry point in pyproject.toml.
        files:
          - "app/babel_tower/pipeline.py"
          - "app/babel_tower/cli.py"
          - "tests/test_pipeline.py"
        test: "uv run pytest tests/test_pipeline.py"
        dod: "Pipeline wired end-to-end, CLI subcommands work, tests pass"

  - name: "Wave 3: Daemon + MCP + Container"
    depends_on: ["Wave 2"]
    tasks:
      - id: T10
        title: "Voice daemon"
        description: |
          Continuous VAD listening loop. Speech→silence triggers pipeline.
          Desktop notifications for state transitions (listening, recording,
          processing, done). Graceful shutdown on SIGINT/SIGTERM.
        files:
          - "app/babel_tower/daemon.py"
        test: "uv run python -c 'from babel_tower.daemon import VoiceDaemon'"
        dod: "Daemon runs continuous listen loop, triggers pipeline on speech"

      - id: T11
        title: "MCP server"
        description: |
          FastMCP server with STDIO transport. Two tools:
          listen (record + process + return text, optional mode parameter),
          set_mode (change default processing mode for session).
          Tool annotations with readOnlyHint=false, destructiveHint=false.
        files:
          - "app/babel_tower/mcp_server.py"
        test: "uv run python -c 'from babel_tower.mcp_server import mcp'"
        dod: "MCP server importable, tools registered, STDIO transport configured"

      - id: T12
        title: "Graceful degradation"
        description: |
          M5 timeout handling: 5s timeout, fallback to raw transcript on
          failure. Notification warns when M5 is unreachable. STT failure
          returns clear error message, no crash. Extend processing.py
          and pipeline.py with try/except and fallback paths.
        files:
          - "app/babel_tower/processing.py"
          - "app/babel_tower/pipeline.py"
        test: "uv run pytest tests/test_pipeline.py"
        dod: "M5 offline → raw transcript returned with warning, no crash"

      - id: T13
        title: "App Docker container"
        description: |
          Create Dockerfile.app: Python 3.13 slim, PulseAudio client,
          PortAudio, wl-clipboard, libnotify. Extend docker-compose.yml
          with daemon service. Document MCP registration command.
          Support both MCP mode (docker run -i) and Daemon mode (compose up).
        files:
          - "docker/Dockerfile.app"
          - "docker/docker-compose.yml"
        test: "docker build -f docker/Dockerfile.app -t babel-tower-app ."
        dod: "App container builds, both MCP and Daemon entry modes work"
